








import numpy as np
import scipy
from scipy.special import expit
from scipy.special import logsumexp
from tqdm import tqdm

class BinaryLogisticLoss():
    """
    Loss function for binary logistic regression.
    It should support l2 regularization.
    """

    def __init__(self, l2_coef):
        """
        Parameters
        ----------
        l2_coef - l2 regularization coefficient
        """
        self.l2_coef = l2_coef

    def func(self, X, y, w):
        """
        Get loss function value for data X, target y and coefficient w; w = [bias, weights].

        Parameters
        ----------
        X : numpy.ndarray
        y : 1d numpy.ndarray
        w : 1d numpy.ndarray

        Returns
        -------
        result : float
        """
        norm = np.linalg.norm(w[1:])
        reg_coeff = norm*norm*self.l2_coef
        X_new = np.c_[np.ones(X.shape[0]), X]
        result_vector = np.logaddexp(0,-y*(X_new.dot(w)))
        result = np.average(result_vector) + reg_coeff
        return result

    def grad(self, X, y, w):
        """
        Get loss function gradient for data X, target y and coefficient w; w = [bias, weights].

        Parameters
        ----------
        X : numpy.ndarray
        y : 1d numpy.ndarray
        w : 1d numpy.ndarray

        Returns
        -------
        : 1d numpy.ndarray
        """
        gradReg = 2 * self.l2_coef * w
        gradReg[0] = 0  

        X_new = np.c_[np.ones(X.shape[0]), X]
        sigmoid = expit(-y * np.dot(X_new, w))
        grad = np.dot(X_new.T, -y * sigmoid) / X.shape[0]
        grad += gradReg

        return grad


loss_function = BinaryLogisticLoss(l2_coef=1.0)
X = np.array([
    [1, 2],
    [3, 4],
    [-5, 6]
])
y = np.array([-1, 1, 1])
w = np.array([1, 2, 3])
assert np.isclose(loss_function.func(X, y, w), 16.00008, atol=1e-5)

loss_function = BinaryLogisticLoss(l2_coef=0.0)
X = np.array([
    [10 ** 5],
    [-10 ** 5],
    [10 ** 5]
])
y = np.array([1, -1, 1])
w = np.array([1, 100])
assert np.isclose(loss_function.func(X, y, w), 0, atol=1e-5)

loss_function = BinaryLogisticLoss(l2_coef=0.0)
X = np.array([
    [10 ** 2],
    [-10 ** 2],
    [10 ** 2]
])
y = np.array([-1, 1, -1])
w = np.array([1, 100])
assert np.isclose(loss_function.func(X, y, w), 10000.333334, atol=1e-5)

loss_function = BinaryLogisticLoss(l2_coef=1.0)
X = np.array([
    [1, 2],
    [3, 4],
    [-5, 6]
])
y = np.array([-1, 1, 1])
w = np.array([1, 2, 3])
right_gradient = np.array([0.33325, 4.3335 , 6.66634])
assert np.isclose(loss_function.grad(X, y, w), right_gradient, atol=1e-5).all()








import numpy as np
from scipy.special import expit

class LinearModel:
    def __init__(
        self,
        loss_function,
        batch_size=100,
        step_alpha=1,
        tolerance=1e-5,
        max_iter=1000,
        random_seed=0,
        **kwargs
    ):
        """
        Parameters
        ----------
        loss_function : BaseLoss inherited instance
            Loss function to use
        batch_size : int
        step_alpha : float
        tolerance : float
            Tolerace for stop criterio.
        max_iter : int
            Max amount of epoches in method.
        """
        self.loss_function = loss_function
        self.batch_size = batch_size
        self.step_alpha = step_alpha
        self.tolerance = tolerance
        self.max_iter = max_iter
        self.random_seed = random_seed
        
        np.random.seed(random_seed)

    def fit(self, X, y, w_0=None):
        """
        Parameters
        ----------
        X : numpy.ndarray or scipy.sparse.csr_matrix
            2d matrix, training set.
        y : numpy.ndarray
            1d vector, target values.
        w_0 : numpy.ndarray
            1d vector in binary classification.
            Initial approximation for SGD method - [bias, weights]
        """
            
        if w_0 is None:
            w_k = np.zeros(X.shape[1] + 1) # [biapd.read_sql('test_data', 'postgres:///db_name') s, weights]
        else:
            w_k = w_0

        last_loss = 10**5
        for epoch in tqdm(range(self.max_iter)):
            epoch_rand_indices = np.random.permutation(X.shape[0])
            inner_cycle_length = int(np.ceil(X.shape[0] / self.batch_size))

            for i in range(inner_cycle_length):
                start_index = self.batch_size * i
                finish_index = self.batch_size * (i + 1)
                batch_indices = epoch_rand_indices[start_index:finish_index]

                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                gradient = self.loss_function.grad(X_batch, y_batch, w_k)
                w_k -= self.step_alpha*gradient

            loss = self.loss_function.func(X, y, w_k)

            if (abs(last_loss - loss) < self.tolerance):
                print("Converged at epoch:", epoch,"\n")
                break
            last_loss = loss

        self.w = w_k
        pass

    def predict_proba(self, X):
        """
        Parameters
        ----------
        X : numpy.ndarray or scipy.sparse.csr_matrix
            2d matrix, test set.
        Returns
        -------
        : numpy.ndarray
            probs, shape=(X.shape[0], 2)
        """
        X_new = np.c_[np.ones(X.shape[0]), X]
        probs = expit(X_new.dot(self.w))
        return np.vstack([1 - probs, probs]).T
        pass


# обратите внимание, что тут достаточно простой тест
# ниже еще есть проверка для данных из data
X1 = np.random.randint(1, 4, (1000, 10))
X2 = np.random.randint(-4, 0, (1000, 10))
X = np.vstack((X1, X2))
y = np.array([-1] * 1000 + [1] * 1000)
loss_function = BinaryLogisticLoss(l2_coef=0.1)
linear_model = LinearModel(
    loss_function=loss_function,
    batch_size=100,
    step_alpha=1,
    tolerance=1e-4,
    max_iter=1000,
)
linear_model.fit(X, y)
prediction_probs = linear_model.predict_proba(X)
predictions = (prediction_probs > 0.5).astype('int')[:, 1] * 2 - 1
assert np.isclose(predictions, y).all()





# не меняем код
import pandas as pd
pd.options.display.max_columns = 100
pd.options.display.max_rows = 150


data = pd.read_csv('application_train.csv')
data.columns = [
    '_'.join([word.lower() for word in col_name.split(' ') if word != '-']) for col_name in data.columns
]
data.target = data.target.map({0: -1, 1: 1})
data.head(3)


# не меняем код
test_idx = data.sk_id_curr % 10 >= 7
data_dict = dict()
data_dict['tst'] = data.loc[test_idx].reset_index(drop=True)
data_dict['tr'] = data.loc[~test_idx].reset_index(drop=True)

for key, df in data_dict.items():
    print(key, 'shape:', df.shape)


# не меняем код
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline

features = data.select_dtypes(np.number).drop(columns=['target', 'sk_id_curr']).columns

X_tr, X_tst = data_dict["tr"][features].to_numpy(), data_dict["tst"][features].to_numpy()
y_tr, y_tst = data_dict["tr"]["target"].to_numpy(), data_dict["tst"]["target"].to_numpy()


prep = make_pipeline(
    SimpleImputer(strategy='median'),
    StandardScaler()
)

prep.fit(X_tr)

X_tr = prep.transform(X_tr)
X_tst = prep.transform(X_tst)





loss_function = BinaryLogisticLoss(l2_coef=0.1)
# 'batch size': 466,
#  'step': 0.04169903111214461,
#  'tolerance': 6.410582886211261
clf = LinearModel(loss_function=loss_function,batch_size= 466,step_alpha=0.04169903111214461,max_iter= 200, tolerance=10**(-6.410582886211261))


import optuna
storage = optuna.storages.RDBStorage("sqlite:///optuna.db")
from sklearn.metrics import roc_auc_score


def objective(trial):
    batch_size= trial.suggest_int('batch size',400,600)
    step_alpha= trial.suggest_float('step',0.02,0.07)
    # max_iter= trial.suggest_int('iterations',250,400)
    max_iter= 200
    tolerance = trial.suggest_float('tolerance',6,7)
    tolerance = 10**(-tolerance)
    clf = LinearModel(loss_function=loss_function,batch_size= batch_size,step_alpha=step_alpha,max_iter= max_iter, tolerance=tolerance)
    clf.fit(X_tr, y_tr)
    return - roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1])

study = optuna.create_study()



study.optimize(objective, n_trials=10)

study.best_params  # E.g. {'x': 2.002108042}


clf.fit(X_tr, y_tr)



print(roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1]))
assert roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1]) > 0.72





from sklearn.linear_model import LogisticRegression
study = optuna.create_study()



def objectiveSklearn(trial):
    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
    C = trial.suggest_float('C', 1e-3, 100, log=True)
    tol = trial.suggest_float('tol', 1e-5, 1e-2, log=True)
    max_iter = trial.suggest_int('max_iter', 100, 1000)
    l1_ratio = None
    solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'saga']) 
    if penalty == 'l1' and solver == 'lbfgs':
        raise optuna.TrialPruned()
    clf = LogisticRegression(
        penalty=penalty,
        C=C,
        solver=solver,
        tol=tol,
        max_iter=max_iter,
        l1_ratio=l1_ratio,
        random_state=42
    )
    
    clf.fit(X_tr, y_tr)
    return -roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1])


study.optimize(objectiveSklearn, n_trials=5)
print(f"Best value: {study.best_value} (params: {study.best_params})")
clf = LogisticRegression()


clf = LogisticRegression(
        penalty='l2',
        C=47.163013827752906,
        solver='liblinear',
        tol=1.0171669120095538e-05,
        max_iter=446,
        l1_ratio=None,
        random_state=42,
        verbose=1
    )
clf.fit(X_tr, y_tr)


assert roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1]) > 0.737


roc_auc_score(y_tst, clf.predict_proba(X_tst)[:, 1])





import seaborn as sns
import matplotlib.pyplot as plt
weights = clf.coef_[0]
feature_names = features
feature_weights = pd.DataFrame({
    'feature': feature_names,
    'weight': weights,
    'abs_weight': np.abs(weights)
})
top_k = 10
top_features = feature_weights.nlargest(top_k, 'abs_weight')


ax = sns.barplot(data=top_features, x='feature', y = 'weight')
ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")








last_features = feature_weights.nsmallest(top_k, 'abs_weight')


display(top_features)


display(last_features)


opt_data = pd.read_sql("SELECT name FROM sqlite_master WHERE type='table';", 'sqlite:///optuna_studies.db')


display(opt_data)
